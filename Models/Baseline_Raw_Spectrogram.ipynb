{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline Raw Spectrogram.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPXLzTXah/lwD3zLBFODt4O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarpat/eeg-mood-classification/blob/master/Models/Baseline_Raw_Spectrogram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHPPrnopQ_Lw",
        "colab_type": "code",
        "outputId": "57a3d971-9549-4b4b-c4ca-89a03853c9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Colab settings/mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My\\ Drive/CSE\\ 240/Project/Data/spectrogram_raw/"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'gdrive/My Drive/CSE 240/Project/Data/spectrogram_raw/'\n",
            "/content/gdrive/My Drive/CSE 240/Project/Data/spectrogram_raw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXwv6XyYRoXy",
        "colab_type": "code",
        "outputId": "3bf4b673-6450-417d-cfa3-55ad2ccde390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Convolution2D, Conv2D, MaxPooling2D, Lambda, GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Activation, AveragePooling2D, Concatenate, SeparableConv2D, MaxPooling3D, Conv3D, DepthwiseConv2D\n",
        "from tensorflow.keras.models import model_from_json"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-RKEd1ERs14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_yBICnpgQPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnvQ22LDR7wB",
        "colab_type": "code",
        "outputId": "1874b838-c07d-434e-f03e-13d6cd2e2aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "base_path = \".\"\n",
        "concentration_data_array = None\n",
        "relaxed_data_array = None\n",
        "print(\"loading data from .npy files...\")\n",
        "for filename in os.listdir(base_path):\n",
        "  if \"E02\" in filename and filename.endswith('.npy') or \"E04\" in filename and filename.endswith('.npy'):\n",
        "    print(filename)\n",
        "    concentration_data_array = np.concatenate((concentration_data_array, np.load(filename))) if concentration_data_array is not None else np.load(filename)\n",
        "  if \"E01\" in filename and filename.endswith('.npy') or \"E03\" in filename and filename.endswith('.npy'):\n",
        "    print(filename)\n",
        "    relaxed_data_array = np.concatenate((relaxed_data_array, np.load(filename))) if relaxed_data_array is not None else np.load(filename)\n",
        "print(concentration_data_array.shape, relaxed_data_array.shape)\n",
        "print(len(concentration_data_array), len(relaxed_data_array))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data from .npy files...\n",
            "S004E01.npy\n",
            "S004E02.npy\n",
            "S004E03.npy\n",
            "S004E04.npy\n",
            "S005E01.npy\n",
            "S005E02.npy\n",
            "S005E03.npy\n",
            "S005E04.npy\n",
            "S006E01.npy\n",
            "S006E02.npy\n",
            "S006E03.npy\n",
            "S006E04.npy\n",
            "S007E01.npy\n",
            "S007E02.npy\n",
            "S007E03.npy\n",
            "S007E04.npy\n",
            "S008E01.npy\n",
            "S008E02.npy\n",
            "S008E03.npy\n",
            "S008E04.npy\n",
            "S009E01.npy\n",
            "S009E02.npy\n",
            "S009E03.npy\n",
            "S009E04.npy\n",
            "S010E01.npy\n",
            "S010E02.npy\n",
            "S010E03.npy\n",
            "S010E04.npy\n",
            "S011E01.npy\n",
            "S011E02.npy\n",
            "S011E03.npy\n",
            "S011E04.npy\n",
            "S012E01.npy\n",
            "S012E02.npy\n",
            "S012E03.npy\n",
            "S012E04.npy\n",
            "S013E01.npy\n",
            "S013E02.npy\n",
            "S013E03.npy\n",
            "S013E04.npy\n",
            "S014E01.npy\n",
            "S014E02.npy\n",
            "S014E03.npy\n",
            "S014E04.npy\n",
            "S015E01.npy\n",
            "S015E02.npy\n",
            "S015E03.npy\n",
            "S015E04.npy\n",
            "S016E01.npy\n",
            "S016E02.npy\n",
            "S016E03.npy\n",
            "S016E04.npy\n",
            "S017E01.npy\n",
            "S017E02.npy\n",
            "S017E03.npy\n",
            "S017E04.npy\n",
            "S018E01.npy\n",
            "S018E02.npy\n",
            "S018E03.npy\n",
            "S018E04.npy\n",
            "S019E01.npy\n",
            "S019E02.npy\n",
            "S019E03.npy\n",
            "S019E04.npy\n",
            "S020E01.npy\n",
            "S020E02.npy\n",
            "S020E03.npy\n",
            "S020E04.npy\n",
            "S021E01.npy\n",
            "S021E02.npy\n",
            "S021E03.npy\n",
            "S021E04.npy\n",
            "S022E01.npy\n",
            "S022E02.npy\n",
            "S022E03.npy\n",
            "S022E04.npy\n",
            "S023E01.npy\n",
            "S023E02.npy\n",
            "S023E03.npy\n",
            "S023E04.npy\n",
            "S024E01.npy\n",
            "S024E02.npy\n",
            "S024E03.npy\n",
            "S024E04.npy\n",
            "S025E01.npy\n",
            "S025E02.npy\n",
            "S025E03.npy\n",
            "S025E04.npy\n",
            "S026E01.npy\n",
            "S026E02.npy\n",
            "S026E03.npy\n",
            "S026E04.npy\n",
            "S027E01.npy\n",
            "S027E02.npy\n",
            "S027E03.npy\n",
            "S027E04.npy\n",
            "S001E01.npy\n",
            "S001E02.npy\n",
            "S001E03.npy\n",
            "S001E04.npy\n",
            "S002E03.npy\n",
            "S002E04.npy\n",
            "S003E01.npy\n",
            "S003E02.npy\n",
            "S003E03.npy\n",
            "S003E04.npy\n",
            "S002E01.npy\n",
            "S002E02.npy\n",
            "S029E04.npy\n",
            "S030E01.npy\n",
            "S030E02.npy\n",
            "S030E03.npy\n",
            "S030E04.npy\n",
            "S028E01.npy\n",
            "S028E02.npy\n",
            "S028E03.npy\n",
            "S028E04.npy\n",
            "S029E01.npy\n",
            "S029E02.npy\n",
            "S029E03.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-2ca105694525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m\"E01\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"E03\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mrelaxed_data_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelaxed_data_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrelaxed_data_array\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcentration_data_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelaxed_data_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcentration_data_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelaxed_data_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 4 dimension(s)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFFV5g9VS7Mj",
        "colab_type": "code",
        "outputId": "c458b9b0-28fd-486d-e3c4-625a7c9685aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = np.concatenate((relaxed_data_array, concentration_data_array))\n",
        "Y = np.concatenate((np.zeros((len(relaxed_data_array),1)),np.ones((len(concentration_data_array),1))))\n",
        "print(X.shape, Y.shape)\n",
        "relaxed_data_array, concentration_data_array = None, None"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4320, 14, 65, 45) (4320, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itoUDpk0TNZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
        "X, Y = None, None\n",
        "folds = list(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmMFvfjOTTwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"4-layer-Convolutions-raw\"\n",
        "def get_model():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(64, kernel_size=3, strides=(1,1), padding='valid', activation='relu', input_shape = (14, 65, 45), data_format='NCHW')) #change the data format as the 14 channels are before the X,Y axes\n",
        "  \n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  sgd = optimizers.SGD(lr=0.000001, momentum=0.7, decay=1e-6)\n",
        "  model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yTgaLafT8-X",
        "colab_type": "code",
        "outputId": "08df68b7-99eb-4cfe-be88-414870086f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "filepath= model_name + \"-weights-improvement-{epoch:02d}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "model = get_model()\n",
        "for j, (train_idx, val_idx) in enumerate(folds):\n",
        "  print('\\nFold ',j)\n",
        "  X_train_cv = X_train[train_idx]\n",
        "  y_train_cv = y_train[train_idx]\n",
        "  X_valid_cv = X_train[val_idx]\n",
        "  y_valid_cv= y_train[val_idx]\n",
        "  # checkpoint\n",
        "  model.fit(X_train_cv, y_train_cv, validation_data=(X_valid_cv, y_valid_cv), epochs=10, batch_size=32, shuffle=True)\n",
        "score = model.evaluate(X_test,y_test)\n",
        "print(\"Test set metrics: %s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
        "model_json = model.to_json()\n",
        "with open(model_name + \".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(model_name + \".h5\")\n",
        "print(\"Saved model %s to disk\" % (model_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-9952259d0c57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nFold '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-7d3bb0cb8a12>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NCHW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#change the data format as the 14 channels are before the X,Y axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m       raise ValueError('Causal padding is only supported for `Conv1D`'\n\u001b[1;32m    132\u001b[0m                        'and ``SeparableConv1D`.')\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     self.dilation_rate = conv_utils.normalize_tuple(\n\u001b[1;32m    135\u001b[0m         dilation_rate, rank, 'dilation_rate')\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36mnormalize_data_format\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    193\u001b[0m     raise ValueError('The `data_format` argument must be one of '\n\u001b[1;32m    194\u001b[0m                      \u001b[0;34m'\"channels_first\", \"channels_last\". Received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                      str(value))\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The `data_format` argument must be one of \"channels_first\", \"channels_last\". Received: NCHW"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx3fpPOMUP_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load json and create model\n",
        "json_file = open(model_name+ '.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(model_name + \".h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X_test,y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOirQT_AKxEP",
        "colab_type": "code",
        "outputId": "1cdcc7dd-0b6a-4e50-ea1e-8c6aa86eaeb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "a = np.array([[[1,2,3,4],[11,22,33,44],[111,222,333,444]], [[5,6,7,8],[55,66,77,88],[555,666,777,888]]])\n",
        "print(a.shape)\n",
        "print(a)\n",
        "\n",
        "b = a.flatten()[:12]\n",
        "for i in range(1, 2):\n",
        "  start = i * 12\n",
        "  end = (i + 1) * 12\n",
        "  b = \n",
        "\n",
        "print(a.flatten()[:12].reshape(3,4))\n",
        "print(a.flatten()[:12].reshape(3,4).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3, 4)\n",
            "[[[  1   2   3   4]\n",
            "  [ 11  22  33  44]\n",
            "  [111 222 333 444]]\n",
            "\n",
            " [[  5   6   7   8]\n",
            "  [ 55  66  77  88]\n",
            "  [555 666 777 888]]]\n",
            "[[  1   2   3   4]\n",
            " [ 11  22  33  44]\n",
            " [111 222 333 444]]\n",
            "(3, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBIxhBVQMVKH",
        "colab_type": "code",
        "outputId": "5583f720-5a84-4ff7-e5e9-4f9f5dcc4089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "#print(np.hstack((X_test[0])).shape)\n",
        "for item in X_test[0]:\n",
        "  print(item.flatten().shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n",
            "(2925,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}